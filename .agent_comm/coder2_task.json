{
  "agent_id": "coder2",
  "task_id": "task_5",
  "files": [
    {
      "filename": "tests/test_group_inference.py",
      "purpose": "Unit tests for group inference engine components",
      "priority": "medium",
      "dependencies": [
        "pytest",
        "torch",
        "numpy"
      ],
      "key_functions": [
        "test_qip_solver",
        "test_scoring_functions",
        "test_progressive_pruning",
        "test_pipeline_integration"
      ],
      "estimated_lines": 200,
      "complexity": "low"
    },
    {
      "filename": "configs/flux_dev_config.yaml",
      "purpose": "Configuration file for FLUX.1 Dev model parameters",
      "priority": "medium",
      "dependencies": [],
      "key_functions": [],
      "estimated_lines": 100,
      "complexity": "low"
    },
    {
      "filename": "configs/scoring_config.yaml",
      "purpose": "Configuration for scoring functions and diversity metrics",
      "priority": "medium",
      "dependencies": [],
      "key_functions": [],
      "estimated_lines": 80,
      "complexity": "low"
    },
    {
      "filename": "visualization_utils.py",
      "purpose": "Utilities for visualizing results, intermediate predictions, and analysis plots",
      "priority": "low",
      "dependencies": [
        "matplotlib",
        "seaborn",
        "PIL",
        "numpy"
      ],
      "key_functions": [
        "plot_image_grid",
        "visualize_pruning_process",
        "plot_correlation_analysis",
        "save_comparison_figure"
      ],
      "estimated_lines": 300,
      "complexity": "low"
    }
  ],
  "project_info": {
    "project_name": "ScalableGroupInference_Diffusion",
    "project_type": "computer_vision",
    "description": "A scalable group inference framework for diffusion models that jointly optimizes diversity and quality of generated image sets by formulating selection as a quadratic integer programming problem with progressive pruning using intermediate denoising predictions.",
    "key_algorithms": [
      "Quadratic Integer Programming",
      "Progressive Pruning Strategy",
      "Intermediate Prediction Correlation",
      "DINOv2 Feature Extraction",
      "CLIP Score Computation"
    ],
    "main_libraries": [
      "torch",
      "torchvision",
      "transformers",
      "diffusers",
      "gurobipy",
      "opencv-python",
      "pillow",
      "numpy",
      "scipy",
      "tqdm",
      "matplotlib",
      "seaborn",
      "einops",
      "accelerate"
    ]
  },
  "paper_content": "PDF: cs.LG_2508.15773v1_Scaling-Group-Inference-for-Diverse-and-High-Quali.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nScaling Group Inference for\nDiverse and High-Quality Generation\nGaurav Parmar1Or Patashnik2,3Daniil Ostashev2Kuan-Chieh Wang2Kfir Aberman2\nSrinivasa Narasimhan1Jun-Yan Zhu1\n1Carnegie Mellon University2Snap Research3Tel Aviv University\nAbstract\nGenerative models typically sample outputs independently, and recent inference-\ntime guidance and scaling algorithms focus on improving the quality of individual\nsamples. However, in real-world applications, users are often presented with a set\nof multiple images (e.g., 4-8) for each prompt, where independent sampling tends\nto lead to redundant results, limiting user choices and hindering idea exploration.\nIn this work, we introduce a scalable group inference method that improves both\nthe diversity and quality of a group of samples. We formulate group inference as\na quadratic integer assignment problem: candidate outputs are modeled as graph\nnodes, and a subset is selected to optimize sample quality (unary term) while maxi-\nmizing group diversity (binary term). To substantially improve runtime efficiency,\nwe progressively prune the candidate set using intermediate predictions, allowing\nour method to scale up to large candidate sets. Extensive experiments show that our\nmethod significantly improves group diversity and quality compared to independent\nsampling baselines and recent inference algorithms. Our framework generalizes\nacross a wide range of tasks, including text-to-image, image-to-image, image\nprompting, and video generation, enabling generative models to treat multiple\noutputs as cohesive groups rather than independent samples.\n1 Introduction\nRecent advances in generative models, such as diffusion models, have driven significant efforts\nin inference-time guidance and scaling techniques [ 1,2,3]. These methods effectively improve\nvarious aspects of output quality, such as alignment with text prompts or image aesthetics, and offer\nfine-grained controls over the output. However, much recent work primarily focuses on enhancing\nthe quality of individual samples generated in isolation.\nYet, in real-world applications, users are often shown a group of samples rather than just one. For\nexample, many text-to-image platforms [ 4,5] display a grid of four to eight images per prompt by\ndefault, a practice that offers users crucial benefits: more diverse choices regarding layout, lighting,\nand style, and new inspirations and ideas for prompt refinement and local edits. This creates a\ngap between current research, focused on independent samples, and the practical need for diverse,\nhigh-quality groups in content creation workflows. How can we close this gap?\nIn this work, we propose a scalable group inference method to jointly improve the diversity and quality\nof a collection of generated samples. We formulate this task as a quadratic integer programming\nproblem, representing output candidates as graph nodes. From a large set of Mcandidates, we select\na subset of size Kthat maximizes a combination of individual sample quality, as a unary term, and\ngroup diversity as a binary term. However, a direct approach involves running the T-step denoising\nPreprint. Under review.arXiv:2508.15773v1  [cs.CV]  21 Aug 2025\n\n--- Page 2 ---\n\u201c\u2026 spaghetti \u2026 couple ... dancing \u2026\u201d\n\u201c photo of a suitcase\u201dI.I.D.Group\n\u201c\u2026 hand drawing watch \u2026\u201d\u201c ... purple potted plant \u2026\u201dI.I.D.Group\n\u201c\u2026 green skateboard \u2026\u201d\u201ca surreal pizza planet\u201dI.I.D.GroupFigure 1: Scalable Group Inference Results. We show the advantage of our proposed group\ninference method over I.I.D. sampling. While I.I.D. sampling often yields repetitive results for the\nsame prompt, our method generates a more diverse and high-quality collection of outputs. Please see\nour project website for more results.\nprocess for all Mcandidates, resulting in an O(MT)complexity. This is computationally expensive\nfor large MandT(e.g., M=128, T=20). To address this, we introduce an efficient progressive\nselection strategy that leverages intermediate predictions during denoising to iteratively prune the\ncandidate set. This approach is grounded in the insight that these intermediate predictions of the\nfinal output, despite originating from a long denoising chain, serve as effective previews of the final\nimage at each step (Figure 3). This approach achieves a complexity of O(M+KT), where K= 4,\nenabling us to scale up our group inference to handle large candidate sets.\nExtensive experiments have shown that our group inference method significantly outperforms inde-\npendent sampling baselines and recent single-sample inference algorithms across various generative\ntasks and modalities, including text-to-image, image-to-image, image prompting, and video genera-\ntion. Our method scales much better and produces more diverse and realistic outputs given the same\ncompute budget. We further provide a comprehensive ablation study demonstrating the effectiveness\nof our design choices. Our framework enables generative models to treat multiple outputs as cohesive\ngroups, aligning more closely with real-world workflows. In summary, our contributions are:.\n\u2022We propose a new, scalable group inference algorithm by selecting a group of K samples from M\ncandidates as a quadratic integer programming problem to maximize sample quality and group\ndiversity.\n\u2022We introduce a progressive pruning strategy to further scale our method. Our technique uses\nintermediate x0predictions as previews to iteratively prune candidates, reducing complexity from\nO(MT)toO(M+KT), where Kis much smaller than M.\n2\n\n--- Page 3 ---\n\u2022Extensive evaluation on text-to-image, image-to-image, image prompting, and video generation\nshows our method outperforms baselines, producing more diverse and realistic outputs within\nsimilar cost budgets.\n2 Related Works\nDiffusion models. are a powerful class of generative models that synthesize high-quality samples\nthrough iterative denoising [ 6,7,8]. Successful in text-to-image synthesis [ 9,10,11], their application\nlater extends to video [ 12,13,14] and 3D synthesis [ 15,16,17]. However, common strategies to\nimprove individual quality, such as fine-tuning for high quality, less diverse datasets [ 9] or strong\nclassifier-free guidance (CFG) [ 1], often sacrifice diversity [ 18]. Additional conditioning methods\nlike spatial controls [ 19] or image prompting [ 20] improve controllability but also reduce diversity,\nespecially with strong guidance values. This lack of diversity is further worsened by one-step or few-\nstep generators [ 21,22,23,24]. Our work addresses this trade-off with group inference, enhancing\nboth sample quality and diversity in batches, and demonstrating applicability across various controls\n(text, spatial, visual prompts).\nDiffusion Inference and Guidance. Inference-time guidance effectively improves sample quality\nand controllability of diffusion models without costly model finetuning. Early methods, such as\nclassifier guidance [ 25] and widely-used classifier-free guidance (CFG) [ 1], significantly increase\nsample quality, often at the cost of diversity. Recent approaches manipulate internal representations,\nsuch as cross-attention maps [ 3], or incorporate spatial control from inputs such as layouts or sketches\n[26,27,28,29,30]. Other strategies apply guidance over limited intervals [ 31] or thresholding to\nCFG to reduce saturation [11].\nWhile the above techniques focus on improving individual samples, our group inference approach\nexplicitly optimizes collective properties, balancing single-sample quality and inter-sample diversity.\nA closely related work is particle guidance [ 32], which incorporates a pairwise potential during\ndenoising steps to encourage diversity. Our method differs in three ways. First, our method improves\nboth quality and diversity, while particle guidance often hurts image quality, as shown in experiments\n(Section 4.2). Second, our method scales effectively to a large number of images through early\ncandidate pruning and sample selection, avoiding expensive optimization. In contrast, particle\nguidance is limited to small sets (e.g., four images) due to memory-intensive gradient computation\nof the pairwise terms. Third, our framework supports non-differentiable quality and diversity terms,\nenabling the use of metrics derived from multimodal LLMs.\nInference-time Scaling. Test-time scaling, leveraging methods like chain-of-thought [ 33], proposer\nand verifier [ 34], or multi-step reasoning, has become a key research area for large-language mod-\nels [35]. The idea is to increase inference-time computation in exchange for improved performance\nfrom a pre-trained model. Recently, researchers have adopted the inference-time scaling for diffusion\nmodels [ 2], which uses off-the-shelf models and evaluation metrics to search for better noises and\nincrease the sample quality, often requiring thousands to tens of thousands of function evaluations\n(NFEs). However, text-to-image models differ from LLMs in three ways: they are often more\ncomputationally expensive [ 36], users often pay 5 to 10 cents per image on leading platforms, and\nusers demand low latency. In our work, we show that our test-time scaling method balances the\ncomputational cost and quality and diversity improvement.\n3 Method\nWe propose Scalable Group Inference , a test-time selection framework that chooses a diverse, high-\nquality subset from a large pool of generated outputs. The method relies on a scoring objective that\ncombines a unary term that measures the quality of an individual sample and a binary term that\ncomputes pairwise properties such as image distances. We first formulate this as a quadratic integer\nprogramming problem (QIP) over binary selection variables. Then, to reduce compute cost, we\nintroduce a progressive filtering strategy that prunes low-quality candidates early using intermediate\npredictions from partially denoised samples. We now describe both components in detail.\n3\n\n--- Page 4 ---\nt=Tt=0t=T-1M/2 Candidates\nM CandidatesKOutputs\n\ud835\udf16!QIPdenoise and drop\nFigure 2: Overview. Given a large number of Mcandidate noises, we gradually reduce the number\nof candidate sets through iterative denoising and pruning steps. At each step, we first leverage\nthe diffusion model \u03f5\u03b8to denoise the sample. We then compute the quality metric (unary term)\nand pairwise distances (binary term), and solve a quadratic integer programming (QIP) program to\nprogressively prune the candidate set, yielding a final group of Kdiverse and high-quality outputs.\n3.1 Formulation\nGiven a generative model G\u03b8(z,c)that maps latent noise z\u223cp(z)and condition cto outputs x, our\ngoal is to obtain a setofKoutputs, {x(i)}K\ni=1, that exhibits both high quality and diversity together.\nWe begin by generating a large set of Mcandidate outputs {x(i)}M\ni=1using i.i.d. sampling:\nx(i)=G\u03b8(z(i),c),z(i)i.i.d.\u223cp(z). (1)\nLetI={1, . . . , M }index the candidate samples. We associate each sample i\u2208 Iwith a unary score\nui\u2208R(e.g., CLIPScore [ 37] between image CLIP embedding and input caption text embedding)\nand each pair (i, j)with a binary score bij\u2208R(e.g., DINO [ 38] distances between two images).\nConcretely,\nui=fCLIP(x(i),c) (2)\nbij= 1\u2212cosine\u0010\nfDINO(x(i)), fDINO(x(j))\u0011\n(3)\nwhere fCLIPcomputes the similarity between the input image and the target caption, and fDINO is the\nDINOv2 feature extractor. Note that our method is general and accommodate many different choices\nof score functions as discussed later in Section 4.4.\nWe introduce binary selection variables yi\u2208 {0,1}where yi= 1 indicates that candidate iis\nincluded in the next group. We define the group selection objective as:\nmax\ny\u2208{0,1}MX\ni\u2208Iuiyi+\u03bbX\ni,j\u2208I\ni<jbijyiyj\nsubject toX\ni\u2208Iyi=K. (4)\n\u03bbis the hyperparameter that controls the relative weight between the unary and binary scores. The\nfirst term rewards individually strong outputs; the second promotes diversity by favoring dissimilar\npairs. Solving this quadratic integer program (QIP) yields a subset of size Kwith desirable group\nproperties. We use the branch-and-cut algorithm implemented by an off-the-shelf solver [ 39] to solve\nthe QIP. Note that the formulation is model-agnostic and can accommodate any scoring functions,\nincluding functions that are not differentiable.\n3.2 Progressive Pruning for Efficient Selection\nNaively applying group selection requires generating all Mcandidates to completion, which is\nprohibitively expensive for recent compute-intensive models like Flux [ 40]. For example, generating\nM= 64 samples over T= 20 denoising steps requires M\u00b7Tforward passes. Even on a modern\nGPU like NVIDIA H100, this results in a runtime of more than 3 minutes. To reduce this cost, we\nintroduce a progressive filtering strategy that prunes candidates early using intermediate predictions.\n4\n\n--- Page 5 ---\nTimestepsScore CorrelationTimestepsScore Correlation\n\ud835\udc65!\u223c\ud835\udc5d(\ud835\udc67)\ud835\udc65\"\ud835\udc65'\".$%\ud835\udc65'\".%\ud835\udc65'\".&%Unary Score Correlation at different Intermediate predictionsBinary Score Correlation at different Intermediate predictionsFLUX.1 Schnell Sampling Trajectory\nFLUX.1 Dev Sampling Trajectory(Noise)(Sample)Figure 3: Correlation Between Intermediate and Final Generation Scores. On the left, we show\nthe reverse diffusion process, visualizing the intermediate predictions \u02c6xtof the final image at different\nsteps for FLUX.1 Schnell and FLUX.1 Dev models. We can observe that the intermediate predictions\nlook similar to true final sample x0for both the models. We further demonstrate this quantitatively\nby plotting the Spearman correlation of the Unary and Binary scores from \u02c6xtversus final x0scores,\nacross different steps. For multistep models like FLUX.1 Dev and Stable Diffusion 3, the plots\ndemonstrate strong correlations rapidly approaching 1.0, even at early timesteps. Note that for a\ntimestep distilled model like Flux-Schnell, the correlation is high from the first denoising step. This\nhighlights the utility of using intermediate predictions for progressively filtering candidate samples.\nIntermediate pruning. We maintain a set St\u2286 I of candidate indices at each step t. For each\nsample in St, we compute the intermediate prediction \u02c6xt, evaluate the unary and binary scores, and\nsolve the QIP (Eq. 4) to select the best subset. This subset becomes the next set St\u22121, forming a\nnested sequence:\nST\u2283 ST\u22121\u2283 \u00b7\u00b7\u00b7 \u2283 S 0.\nOnce the set reaches the desired output group size K, we stop the pruning and complete the remaining\ndenoising steps only for the selected samples. See Algorithm 1 for the full procedure.\nReliability of early predictions. In modern multi-step diffusion and flow-based models, the\nintermediate state xtalready encodes coarse information about the final generated sample x0. A\ncommon approximation of the final image at timestep tis the predicted reconstruction:\n\u02c6xt=xt+t\u00b7\u03f5\u03b8(xt, t,c), (5)\nwhere \u03f5\u03b8predicts the noise or velocity at time t. Although these predictions are coarse, they are\nsufficient for computing the unary and binary scoring functions.\nTo quantify this, we compute the correlation between the scoring functions (e.g., CLIP similarity or\npairwise DINO diversity) evaluated on the intermediate images \u02c6xtand the final output x0. Across a\nrange of denoising steps, Figure 3 (right) shows strong correlations (e.g., r >0.7after 5 steps for\nmulti-step models, and r >0.95after the first step for distilled models), indicating that intermediate\npredictions are reliable proxies. Figure 3 (left) shows this visually. This high correlation enables us\nto safely rank candidates before they are fully denoised.\n3.3 Computational Complexity Analysis\nTo analyze the efficiency of progressive filtering, suppose we start with Mcandidate samples and\nprune the set by a fixed ratio \u03c1\u2208(0,1)at each denoising step until reaching a target set size K. Thus,\nthe number of candidates at timestep tis given by:\n|St|= max\u0000\n\u03c1tM, K\u0001\n. (6)\nLetTdenote the total number of denoising steps. We define the timestep t\u2217at which the candidate\nset size first reaches or falls below the target K:\nt\u2217=\u0018log(K/M )\nlog(\u03c1)\u0019\n. (7)\nThe total number of model evaluations f\u03b8required throughout the process can be written as:\nM\u00b71\u2212\u03c1t\u2217\n1\u2212\u03c1+K\u00b7(T\u2212t\u2217+ 1). (8)\n5\n\n--- Page 6 ---\nAlgorithm 1 Efficient group inference\n# model: The diffusion model \u03f5\u03b8\n# zs: Initial noise vectors {z_i}\n# N: Total number of denoising steps\n# ts: The noise schedule {t_j}\n# K: The target number of samples\n# c: Conditioning information\n# rho: The dropping ratio \u03c1for pruning\ndef group_inference(model, zs, N, ts, K, c, rho):\n# Initialize the set of candidates from noise\ncandidate_set = list(zs)\n# Denoising loop\nfor j in reversed(range(1, N +1)):\nintermediate_previews, next_latents = [], []\n# Get intermediate previews for all candidates\nfor x_t in candidate_set:\npreview, x_next = denoise(x_t, ts[j], ts[j-1], c, model)\npreviews.append(preview)\nnext_latents.append(x_next)\nif len(candidate_set) > K:\n# Score previews and select the best subset\nu = unary_score(previews)\nb = binary_score(previews)\n# Prune candidates based on the dropping ratio rho\nm = max(K, int(len(candidate_set) * (1 - rho)))\nindices = SolveQIP(u, b, m)\ncandidate_set = [next_latents[i] for i in indices]\nelse:\ncandidate_set = next_latents\nreturn candidate_set\nIn contrast, naive sampling without pruning would require M\u00b7Tmodel evaluations. For typical\nparameter settings (e.g., M= 64 ,K= 4,\u03c1= 0.5,T= 20 ), our progressive filtering approach\nyields substantial compute savings (i.e., 184vs1280 evaluations, \u223c85% reduction). Our method has\nan overall complexity of O(M+KT).\n4 Experiments\nWe demonstrate the effectiveness of our proposed scalable group inference method across three\ndifferent tasks: text-to-image generation, depth-conditioned generation, encoder-based image cus-\ntomization, and five different base models: FLUX.1 Schnell, FLUX.1 Dev, Stable Diffusion 3\n(Medium), FLUX.1 Depth, and SynCD. The dataset and evaluation protocols used throughout all\nexperiments are described next in Section 4.1. Subsequently, in Section 4.2 and Section 4.3, we\ncompare against prior methods along two axes: diversity-quality tradeoff, and inference-time scala-\nbility with different compute budget constraints. Finally, we present an ablation study to analyze the\ndifferent components of our method, a runtime analysis, and their respective contributions. Please\nsee the Appendix B, A, C for more results, analysis and ablations.\n4.1 Dataset and Evaluation\nDatasets. We use the GenEval dataset [ 41], validation split of the COCO 2017 dataset [ 42], and\nDreamBooth dataset [ 43] for text-to-image generation, depth-conditioned generation, and image\n6\n\n--- Page 7 ---\nI.I.D.Group\u201cA photo of a computer mouse.\u201d\u201cA photo of a broccoli and a vase.\u201d\n\u201cA photo of an apple.\u201d\u201cA photo of a tie.\u201dI.I.D.Group\u201cA photo of a boat.\u201d\u201cA photo of a skateboard.\u201dI.I.D.GroupI.I.D.Group\u201cA photo of a couch and a horse.\u201d\u201cA photo of three refrigerators.\u201d\nFigure 4: Gallery of Results. Qualitative results that show the advantage of our proposed group\ninference method over I.I.D. sampling for text-to-image generation and depth-to-image generation.\nTop row shows results with FLUX.1 Schnell, the second row uses FLUX.1 Dev, and the last two\nrows use FLUX.1 Depth as the base model. For text-to-image generation, our method produces more\ndiverse object poses and orientations, while for depth-to-image generation, it enhances color and\ntexture diversity while adhering to the input depth condition.\ncustomization, respectively. For depth-conditioned generation, we first extract the depth map using a\nrecent method [44]. Please refer to Appendix C for additional details.\n7\n\n--- Page 8 ---\nIndividual Sample Quality(Unary Score)Set Diversity(Binary Score)FLUX.1 Dev\nIndividual Sample Quality(Unary Score)Set Diversity(Binary Score)FLUX.1 Schnell\nIndividual Sample Quality(Unary Score)Set Diversity(Binary Score)SD3 (M)\nIncreasing stepsVarying CFG\nInterval Guidance\nParticle Guidance\nGroup Inference(ours)Figure 5: Quality and Diversity Pareto front for text-to-image models. Each curve corresponds to\na different inference strategy for three different text-to-image models (FLUX.1 Dev, FLUX.1 Schnell,\nand Stable Diffusion 3 Medium). Our proposed Group Inference ( blue) consistently dominates alter-\nnate methods (Increasing steps, Varying CFG, Interval Guidance, and Particle Guidance) achieving\nPareto optimality and superior tradeoffs between quality and diversity across all methods. Varying\nCFG and Interval Guidance do not apply to the distilled model (FLUX.1 Schnell).\nModels. We use several recent models, including FLUX.1 Dev [ 40] and Stable Diffusion 3 Medium\n(SD3-M) [ 45], which are flow-based models typically requiring 20-50 denoising steps. We also\nevaluate FLUX.1 Schnell, a timestep-distilled variant designed for efficient generation, typically\nusing 1-8 steps. For depth-conditioned generation, we use FLUX.1 Depth, a model specifically\ntrained for structural guidance based on depth maps. For customization, we use SynCD [ 46], a recent\nencoder-based image prompting model. Unless otherwise specified, the sampling parameters for\nthese models are fixed to the default values. A comprehensive list and inference parameters used can\nbe found in Appendix C.1.\nScore Functions. We use CLIP text-image similarity [ 47] to assess the quality of the individual\nsamples (unary score) for the text-to-image and depth-to-image generation. For encoder-based image\ncustomization, we use cosine DINOv2 [ 38] similarity between the input subject image and the output\ngenerated images for the unary score. Diversity (binary score) is computed for all tasks as one minus\nthe cosine similarity between the DINOv2 patchwise features of all image pairs in the output set. Our\nmethod can naturally accommodate a wide range of unary and binary scores, fitting the user\u2019s needs.\nSection 4.4 demonstrate this concretely.\nUser Study. We conduct two user preference studies to compare our method against each baseline\non text-to-image generation. The first user study evaluates output diversity. In each comparison,\nthe users are presented with two sets of 4 output images generated by two methods. The users are\ninstructed to choose the set that has the higher variety. The second user study evaluates individual\nsample quality. For this study, the users are shown two images generated by two methods and asked\nto pick the one with higher quality. Both studies were conducted using Amazon Mechanical Turk\n(AMT) using prompts from our entire validation set. Each comparison was rated by three unique\nusers, resulting in a total of 23,226 preference judgments.\nRuntime. We measure inference runtime using wallclock time. Specifically, this is the time taken\nby each method to generate an output set of Kimages (where K= 4, unless specified otherwise)\nfrom a given input condition (i.e., a text prompt, depth map, or subject image). This measurement\nexcludes initial model loading times and is averaged over 20 independent runs for each reported\nvalue. All runtime experiments utilize a single NVIDIA H100 GPU. Runtime comparisons based on\nthe number of function evaluations (NFEs) are included in the appendix.\nUncertainty Estimation. We report standard errors for all quantitative results presented throughout\nour experiments. These standard errors are computed via bootstrapping with 1000 resamples.\n4.2 Baselines and the Diversity-Quality Tradeoff\nIn generative modeling, a fundamental tradeoff often exists between optimizing for the perceptual\nquality of individual samples and ensuring a diverse set of outputs [ 48,1,49]. Many prior methods\nimplicitly or explicitly navigate this spectrum. In this subsection, we compare our proposed approach\n8\n\n--- Page 9 ---\nDiversity Quality\nModel Comparison Ours pref. Baseline pref. Ours pref. Baseline pref.\nFLUX.1 DevOurs vs Low-CFG 88.3% 11.70% 85.6% 14.4%\nOurs vs Interval Guidance 53.4% 46.6% 58.4% 41.6%\nOurs vs Particle Guidance 81.2% 18.8% 79.4% 20.6%\nFLUX.1 SchnellOurs vs Low-CFG N/A\nOurs vs Interval Guidance N/A\nOurs vs Particle Guidance 55.5% 44.5% 62.3% 37.7%\nSD3 (M)Ours vs Low-CFG 76.8% 23.2% 80.8% 19.20%\nOurs vs Interval Guidance 58.1% 41.9% 57.9% 42.10%\nOurs vs Particle Guidance 78.9% 21.1% 85.9% 14.1%\nTable 1: User preference comparison between our method and baselines. Results from our user\nstudy demonstrate that our method is consistently preferred over alternative inference strategies.\nAcross three different text-to-image models (FLUX.1 Dev, FLUX.1 Schnell, and Stable Diffusion\n3 Medium), users consistently chose our generations for both diversity and quality. Note that\ncomparisons against Low-CFG and Interval Guidance are not applicable (N/A) for the distilled\nFLUX.1 Schnell model.\nIndividual Sample Quality(Unary Score)Set Diversity(Binary Score)FLUX.1 Depth\nIndividual Sample Quality(Unary Score)Set Diversity(Binary Score)SynCD\nIncreasing steps\nVarying CFG\nInterval Guidance\nParticle Guidance\nGroup Inference (ours)\nFigure 6: Quality and Diversity Pareto front for additional tasks. Each curve corresponds to\na different inference strategy for depth conditioned generation (left, FLUX.1 Depth) and image\nprompting (right, SynCD). Our proposed Group Inference ( blue) consistently dominates alternative\nmethods\u2014Increasing steps, Varying CFG, Interval Guidance, and Particle Guidance\u2014achieving\nPareto optimality and superior tradeoffs between quality and diversity across all methods.\nto existing approaches proposed to achieve a more favorable Pareto frontier in the diversity-quality\nspace. Figure 5 plots the quality and diversity Pareto front for text-to-image models (FLUX.1 Dev,\nFLUX.1 Schnell, and SD3 (M)). Table 1 shows the results of a pairwise user preference study between\nour method and the baselines. Figure 6 plots the quality diversity tradeoff for additional models.\nQualitative comparisons are shown in Figure 7.\nAcross all baselines, our proposed method consistently achieves a superior diversity-quality tradeoff.\nAs illustrated by the blue line in Figures 5 and 6, our approach dominates the Pareto fronts of\nall evaluated baselines, yielding better diversity for a given level of quality, or higher quality for a\ncomparable level of diversity. Comparisons with additional metrics are shown in the Appendix.\nIncreasing Denoising Steps. We first consider the impact of simply increasing the number of\ndenoising steps during sampling. While more steps can sometimes refine details, we find this has a\nminimal effect on meaningfully shifting the diversity-quality balance for the models under study, as\nshown by the redline in Figures 5 and 6.\nVarying CFG. Next, we examine the widely used technique of varying the Classifier-Free Guidance\nscale [ 1] (CFG). As depicted by the orange line in Figures 5 and 6, systematically altering the CFG\nscale traces a distinct tradeoff curve. Notably, low CFG values (e.g., CFG=1) largely increase output\ndiversity but often at the cost of a sharp degradation in sample quality and prompt alignment. This is\n9\n\n--- Page 10 ---\n\u201cA photo of a zebra and a bed.\u201d\nLowCFGOursIntervalGuidanceParticleGuidance\nLowCFGOursIntervalGuidanceParticleGuidance\u201cA photo of cake and a stop sign.\u201dFigure 7: Qualitative results. We compare our proposed method (top row) against alternative\ninference strategies targeting an improved Quality-Diversity tradeoff with FLUX.1 Dev base model.\nTo ensure a fair comparison, baseline methods were configured to approximate the diversity level\nachieved by our approach. The precise parameters of each baseline, and a comparison at other\nconfigurations is shown in the supplement. The result demonstrates that: (i) employing a low\nClassifier-Free Guidance (CFG) scale to increase diversity results in diminished image quality; (ii)\nInterval Guidance exhibits reduced adherence to the input text prompt; and (iii) Particle Guidance, by\nactively altering sampling trajectories, tends to produce less natural images. In contrast, our method\noutputs a set of diverse outputs while maintaining good image quality and prompt fidelity.\nGroup Inference (ours)Increasing Steps\nInference Diffusion Scaling \nRuntime(seconds)Combined ScoreRuntime(seconds)Combined ScoreFLUX.1 SchnellFLUX.1 Dev\nRuntime(seconds)Combined ScoreSD3 (M)\nFigure 8: Performance at different runtimes. The Increasing Steps ( red) baseline shows limited\ngains with additional computation. The Inference Diffusion Scaling [ 2] method ( orange ), which\nincreases sample count through independent I.I.D. generations, requires substantially more runtime\nfor marginal improvements. In contrast, our proposed Group Inference ( blue) achieves significantly\nbetter performance\u2013runtime tradeoffs, quickly outperforming both baselines with minimal overhead.\nObservation holds across models.\nvisually seen through the poor image quality in the second row in Figure 7 where a low CFG value is\nused. The results of the user study in Table 1 further corroborate these observations.\nInterval Guidance. Interval guidance [ 31] attempts to refine this by applying CFG selectively only\nduring a subset of the denoising timesteps. We conduct a sweep across various interval configurations,\nwith the results shown as the green line in Figures 5 and 6. Consistent with the original findings for\nInterval Guidance and third row in Figure 7, this approach can offer image quality improvements over\na standard CFG sweep. However, it still performs worse than our method in terms of both quality and\ndiversity. For instance, in the example of zebra and bed on the left in Figure 7, Interval Guidance has\nreduced diversity of zebra poses and does not generate a bed for two of the four outputs. Similarly, in\nthe right example, interval guidance does not always generate a stop sign. Moreover, both standard\n10\n\n--- Page 11 ---\nI.I.D.M=128M=64M=32M=16M=8Set DiversityIndividual Sample QualityFLUX.1 Dev\nI.I.D.M=128M=64M=32M=16M=8Set DiversityIndividual Sample QualityFLUX.1 Schnell\nI.I.D.M=128M=64M=32M=16M=8Set DiversityIndividual Sample QualitySD3 (M)Figure 9: Improvements as the number of initial samples M is increased. We show how the\nsample quality (measured with CLIP) and the set diversity (measured with DINO) improves as the\nnumber of initial starting samples is increased from 4 to 128.\nCFG sweeping and Interval Guidance do not apply to distilled models like FLUX.1 Schnell, as these\nmodels do not use guidance mechanisms.\nParticle Guidance. We also evaluate Particle Guidance [ 32], which optimizes a binary potential\nfunction to encourage diversity during inference. Following the original work, we use DINO features\nfor the diversity term. As illustrated by the purple line, Particle Guidance can indeed increase output\ndiversity. However, this comes with a sharp decrease in individual sample quality (fourth row in\nFigure 7), as direct optimization of the binary potential actively alters the sampling trajectory. This\ncan push the output samples off the learned data manifold, leading to less natural and artifact-prone\nimages. Furthermore, Particle Guidance requires a substantial memory cost due to the necessity of\ncomputing gradients and backpropagating through the binary potential. This reliance on gradient\ncomputation also makes the method unsuitable for non-differentiable potential functions.\n4.3 Inference Scaling Analysis\nScaling computational resources at the test time to enhance model performance is an increasingly\nuseful paradigm in machine learning. For diffusion models, a native mechanism for test-time scaling\ninvolves increasing the number of denoising steps. Although this can initially lead to improved\nsample quality, this approach often yields diminishing returns; beyond a certain point, additional\ndenoising steps provide progressively smaller gains in quality. In Figure 8, we illustrate the impact of\nvarious test-time scaling strategies on the group objective (defined in Equation 4), evaluated across\nvarious computational budgets.\nOur first baseline (Figure 8, redline) allocates increased compute to a greater number of denoising\nsteps for a fixed number of initial samples M. Consistent with existing findings [ 2], this approach\ndemonstrates minimal improvement in our combined score, with the curve quickly plateauing.\nInference Diffusion Scaling [ 2], proposes an alternative that utilizes the additional compute budget\nto perform a search over multiple random seeds. For a fair comparison, we implement this baseline\nwith the CLIP text-image similarity as the verifier. This method does not incorporate intermediate\npredictions and does not consider any pairwise terms. Consequently, it is not effective in improving\nthe group objective, as shown through the orange line.\nIn contrast, our method invests in the inference budget to increase the number of initial samples.\nAs depicted by the blue line in Figure 8, this approach produces consistent improvements in the\ncombined group score. Figure 9 further shows the improvement in both the quality and diversity of\nthe outputs as the number of initial samples is gradually increased from 4 to 128 across three different\nbase models.\nAblating progressive filtering. The importance of progressive filtering is shown in Figure 10. We\ncompare our complete method, which utilizes progressive filtering with intermediate predictions \u02c6xt\n(redline), against a variant that performs full denoising for all Mcandidate samples without such\nfiltering ( gray line). Demonstrating its effectiveness across different architectures, our approach\nachieved comparable group scores while requiring up to 73% less runtime. Please see the appendix\nfor additional ablation studies.\n11\n\n--- Page 12 ---\nRuntime(seconds)Combined ScoreFLUX.1 Dev49% faster\nRuntime(seconds)Combined ScoreFLUX.1 Schnell73% faster\nRuntime(seconds)Combined ScoreSD3 (M)55% fasterGroup Inference (Ours)\nOurs w/o progressive filteringFigure 10: Importance of progressive pruning. Across multiple base generative models, progressive\npruning consistently enables our method to select candidates efficiently and shows substantial\nspeedups-49%, 73%, and 55% faster for comparable combined group scores.\n\u201dA small alien ... sitting \u2026 cigarette\u2026\u201d\u201cA giantneonrose.\u201dColor DiversityGroupI.I.D.\nDINO Diversity\nFigure 11: Accommodating different pairwise objectives. Compared to baseline I.I.D. sampling\n(top row), our method allows for targeted diversity by defining different pairwise objectives. The\nsecond and third rows show results where the unary quality term is identical but the pairwise binary\nterm is varied. The middle row uses a color-based binary term, while the bottom row uses a DINO-\nbased binary term to achieve semantic and structural diversity.\n4.4 Different Diversity Objectives\nNext, we show that our approach is general and can accommodate different pairwise binary objectives\nby simply swapping the binary term in our quadratic integer programming objective, as demonstrated\nin Figure 11. Let us consider the example shown on the left corresponding to the caption \u201ca giant\nneon rose.\u201d Standard I.I.D. sampling (top row) produces a set of visually redundant images. All four\nroses are red and share a similar pose.\nIn contrast, the bottom two rows are generated using our method with an identical unary quality term\n(CLIP text-image similarity) but different binary diversity objectives. The middle row uses a direct\ncolor based dissimilarity as the binary term. This successfully steers the outputs towards a set of\nimages with varied and distinct color schemes. For the rose example on the left, this results in a\nvibrant set that includes blue, orange, and pink neon variants. In the bottom row, we use a DINO\ndiversity metric that captures more semantic features when comparing the pairwise distances. This\nchange directs the model to produce a set with higher structural variance. As seen with the rose\nexample, this yields outputs with different poses and camera angles.\nThis direct comparison underscores a key strength of our approach: the ability to seamlessly integrate\ndifferent notions of diversity to achieve targeted, user-defined visual outcomes.\n12\n\n--- Page 13 ---\n\u201dA photo of Albert Einstein.\u201d\u201cA photo of a Ferrari.\u201d\nGroupI.I.D.Figure 12: Failure cases. The performance of our method depends on the diversity of the initial\ncandidate pool. (Left) For the prompt \u201cA photo of a Ferrari,\u201d the base model (FLUX.1 Schnell)\nexhibits a strong color bias, exclusively generating red cars. Consequently, our method can find\nvaried poses but is unable to produce a color-diverse set. (Right) Similarly, for \u201ca photo of Albert\nEinstein,\u201d the base model only generates black-and-white images, constraining our method from\nfinding any color photographs.\n5 Discussion, Broader Impacts, and Limitations\nIn this paper, we have introduced scalable group inference, a novel method to generate diverse, high-\nquality sets of samples by formulating the selection as a quadratic integer program and leveraging\nintermediate predictions for improving the runtime efficiency. Our efficient approach significantly\nenhances group diversity and quality compared to existing baselines across various generative tasks.\nStill, our method has several limitations.\nFirst, our method relies on the base generative model\u2019s ability to produce a sufficiently diverse and\nhigh-quality initial candidate pool. Consequently, if the underlying model generates outputs of\ninherently poor quality or suffers from significant mode collapse, the efficacy of Scalable Group\nInference in identifying an optimal set will be inherently constrained, as our method selects from,\nrather than intrinsically enhances, these initial candidates. This is visually illustrated in Figure 12.\nSecond, our method assumes that the unary (quality) and binary (diversity) scores are fast to compute.\nIf evaluating these scores, especially the pairwise diversity metric across a large candidate set, is\ncomputationally intensive, the runtime benefits of our scalable optimization would be reduced.\nNevertheless, our method offers a path to more user-centric systems that efficiently output diverse,\nhigh-quality sets of options, and enhance creative exploration. This capability can significantly\nreduce the iterative burden in content generation across various domains. Concurrently, the increased\nefficiency in generating diverse sets of synthetic media could also have potential for misuse, such\nas creating more varied and potentially harder-to-detect misleading content, demanding proactive\nethical guidelines and mitigation strategies.\nAcknowledgments and Disclosure of Funding\nWe thank Daniel Cohen-Or and Sheng-Yu Wang for their helpful comments and discussion. We are\nalso grateful to Nupur Kumari for proofreading the draft. The project is partially supported by Snap\nResearch, NSF IIS-2239076, DARPA ECOLE, and the Packard Fellowship.\nReferences\n[1]Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep\nGenerative Models and Downstream Applications , 2021.\n[2]Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong\nLi, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond\nscaling denoising steps. 2025.\n[3]Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot\nimage-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings , pages 1\u201311, 2023.\n13\n\n--- Page 14 ---\n[4] Midjourney, Inc. Midjourney Website. https://www.midjourney.com/home , 2024.\n[5]Adobe Inc. Adobe Firefly: Generative AI for Creatives. https://www.adobe.com/products/firefly.\nhtml , 2025.\n[6]Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learning , pages\n2256\u20132265. PMLR, 2015.\n[7]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems , 33:6840\u20136851, 2020.\n[8]Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 , 2020.\n[9]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2022.\n[10] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In\nThe Twelfth International Conference on Learning Representations , 2024.\n[11] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in neural information processing\nsystems , 35:36479\u201336494, 2022.\n[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\nVideo diffusion models. Advances in Neural Information Processing Systems , 35:8633\u20138646, 2022.\n[13] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 22563\u201322575,\n2023.\n[14] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,\nYam Levi, Zion English, Vikram V oleti, Adam Letts, et al. Stable video diffusion: Scaling latent video\ndiffusion models to large datasets. arXiv preprint arXiv:2311.15127 , 2023.\n[15] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988 , 2022.\n[16] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 300\u2013309,\n2023.\n[17] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. arXiv preprint arXiv:2308.16512 , 2023.\n[18] Pietro Astolfi, Marlene Careil, Melissa Hall, Oscar Ma\u00f1as, Matthew Muckley, Jakob Verbeek, Adri-\nana Romero Soriano, and Michal Drozdzal. Consistency-diversity-realism pareto fronts of conditional\nimage generative models. arXiv preprint arXiv:2406.10429 , 2024.\n[19] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3836\u20133847,\n2023.\n[20] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter\nfor text-to-image diffusion models. arXiv preprint arXiv:2308.06721 , 2023.\n[21] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast\nhigh-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024\nConference Papers , pages 1\u201311, 2024.\n[22] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation.\nInEuropean Conference on Computer Vision , pages 87\u2013103. Springer, 2024.\n14\n\n--- Page 15 ---\n[23] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman,\nJun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. In European Conference\non Computer Vision , pages 428\u2013447. Springer, 2024.\n[24] Tianwei Yin, Micha\u00ebl Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and\nTaesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 6613\u20136623, 2024.\n[25] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In\nA. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information\nProcessing Systems , 2021.\n[26] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance.\nInIEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , 2024.\n[27] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation\nwith attention modulation. In IEEE International Conference on Computer Vision (ICCV) , 2023.\n[28] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing.\nInIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2024.\n[29] Andrey V oynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In\nACM SIGGRAPH 2023 conference proceedings , 2023.\n[30] Yutong He, Ruslan Salakhutdinov, and J Zico Kolter. Localized text-to-image generation for free via cross\nattention control. arXiv preprint arXiv:2306.14636 , 2023.\n[31] Tuomas Kynk\u00e4\u00e4nniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen.\nApplying guidance in a limited interval improves sample and distribution quality in diffusion models. arXiv\npreprint arXiv:2404.07724 , 2024.\n[32] Gabriele Corso, Yilun Xu, Valentin De Bortoli, Regina Barzilay, and Tommi Jaakkola. Particle guidance:\nnon-iid diverse sampling with diffusion models. arXiv preprint arXiv:2310.13102 , 2023.\n[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems , 35:24824\u201324837, 2022.\n[34] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be\nmore effective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024.\n[35] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. s1: Simple test-time scaling.\narXiv preprint arXiv:2501.19393 , 2025.\n[36] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng,\nJun-Yan Zhu, and Song Han. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion\nmodels. arXiv preprint arXiv:2411.05007 , 2024.\n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125 , 1(2):3, 2022.\n[38] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V V o, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual\nfeatures without supervision. Transactions on Machine Learning Research , 2023.\n[39] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2025.\n[40] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux , 2024.\n[41] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for\nevaluating text-to-image alignment. Advances in Neural Information Processing Systems , 36:52132\u201352152,\n2023.\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on\nComputer Vision , pages 740\u2013755. Springer, 2014.\n15\n\n--- Page 16 ---\n[43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , pages 22500\u201322510, 2023.\n[44] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything:\nUnleashing the power of large-scale unlabeled data. In CVPR , 2024.\n[45] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi,\nDominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution\nimage synthesis. In Forty-first international conference on machine learning , 2024.\n[46] Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, and Samaneh Azadi. Generating multi-image synthetic\ndata for text-to-image customization. ArXiv , 2025.\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning , pages 8748\u20138763. PMLR,\n2021.\n[48] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli\nShechtman. Toward multimodal image-to-image translation. Conference on Neural Information Processing\nSystems (NeurIPS) , 30, 2017.\n[49] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096 , 2018.\n[50] Ollin Boer Bohan. Tiny autoencoder for stable diffusion. Retrieved May , 22:2024, 2023.\n[51] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagereward: Learning and evaluating human preferences for text-to-image generation. Advances in\nNeural Information Processing Systems , 36:15903\u201315935, 2023.\n[52] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.\nDepth anything v2. Advances in Neural Information Processing Systems , 37:21875\u201321911, 2024.\n[53] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In International conference on machine learning ,\npages 19730\u201319742. PMLR, 2023.\n16\n\n--- Page 17 ---\nSection B presents additional qualitative and quantitative results obtained by our method across multiple different\nbase generative models and tasks. Section A provides additional analysis of the different components of our\nmethod. Section C details the datasets and the implementation settings used for each of the baseline methods.\nA Analysis\nIn this section, we provide additional analysis of the different components of our method.\nRuntime breakdown. In Figure 13 we show the runtime breakdown of different steps in the pipeline for\nthe FLUX.1 Dev base model using CLIP text image similarity as the unary score and DINOv2 diversity as the\nbinary score. On the left, we fix the output set size K to be 4 and increase the initial candidate size from 4 to 200.\nOn the right, we fix the initial candidate size to be 200, and increase the output set size from 4 to 128. Note that\nacross all settings, the runtime cost incurred by the QIP solver and the score computation is negligible compared\nto the forward pass of the denoising transformer.\nDifferent \u03c1values. Figure 14 illustrates the effects of varying pruning ratios ( \u03c1) on the FLUX.1 Dev and\nFLUX.1 Schnell models. The figure presents both the Number of Function Evaluations (NFE) (left plot) and the\nwallclock runtime on a single NVIDIA H100 (right plot). Across all plots, a pruning ratio of \u03c1= 1.0signifies no\nprogressive pruning. For the FLUX.1 Dev model, lower pruning ratios (e.g., \u03c1= 0.1and\u03c1= 0.25) are overly\naggressive, leading to suboptimal scores. Conversely, a pruning ratio of \u03c1= 1.0(no candidate filtering) achieves\na good combined score but incurs a high inference cost. A pruning ratio of \u03c1= 0.5strikes an effective balance,\nyielding higher scores without excessive computational cost. We use \u03c1= 0.5for all FLUX.1 Dev experiments.\nA different trend observed for distilled FLUX.1 Schnell model. It can accommodate a more aggressive pruning\nratio, such as \u03c1= 0.1, without a noticeable decrease in the score. This can be attributed to the better reliability\nof the intermediate predictions for the distilled models, as shown in Figure 3 of the main paper.\nEfficient decoding. Our method uses an efficient decoder [ 50] to decode all intermediate predictions for\nprogressive pruning. In Figure 16 we ablate the use of efficient decoder and show that across both FLUX.1 Dev\nand FLUX.1 Schnell, using an efficient decoder improves the runtime without sacrificing the score.\nEvaluation with different score functions. Figure 5 in the main paper shows the quality and diversity\nPareto front for the text to image generation task. That figure uses CLIP text-image similarity (Equation 2) as the\nquality score and DINO diversity 3 as the diversity score. Next, we evaluate our method using several additional\nscore functions that are not used by our method for selection in Figure 17. The top row uses Image Reward [ 51]\nfor measuring quality of samples and depth features to measure diversity. Image Reward is a network that is\ntrained to learn human preferences for text-to-image generation. The depth diversity is calculated with the\nDepthAnything V2 model [ 52]. The bottom row uses BLIP2 [ 53] to measure the quality and CLIP features\nto compute the diversity. Figure 17 shows a comparison with three different base models: FLUX.1 Dev (left),\nFLUX.1 Schnell (middle), and Stable Diffusion 3 medium (right). Across each model, our proposed group\ninference shows a better trade off between quality and diversity. Note that particle guidance obtains slightly\nbetter BLIP2 score than our method for Stable Diffusion 3 (M). However, the outputs generated by particle\nguidance have artifacts. This is also reflected by a low score for other metrics (Image Reward and CLIP), and a\nworse user preference score.\nB Additional Results\nQualitative results. In Figures 21, 22, and 23 we show additional visual examples of our method. Across\nmultiple models and tasks, our method consistently outputs samples that are more diverse, and without any\ndegradation in the quality. Similar to the Figure 7 in the main paper, Figure 20 shows additional visual comparison\nto baselines. In these figures, the Low CFG baseline uses a CFG value of 1.0. Interval Guidance uses an interval\nof[0.6,0.4], and particle guidance uses a coefficient value of 100.\nCorrelation analysis. Figure 3 in the main paper shows the correlation between the scores computed with\nthe final image and the intermediate images. In Figure 18, we show that a similar correlation trend is visible in\nother base models (FLUX.1 Depth and SynCD). FLUX.1 Depth and SynCD show the CLIP text image similarity\nas the unary scores, and DINO diversity as the binary scores. This is consistent with our observations in the\nmain paper.\nC Implementation Details\nSection C.1 first provides implementation details and hyperparameters used for all settings shown in Figures 4,\n5, and 6 of the main paper. Section C.2 lists details about the datasets used for each task.\n17\n\n--- Page 18 ---\nDenoisingQIP solverUnary ScoreBinary Score\nInitial Candidates(M)Runtime(seconds)\nOutput Size(K)Runtime(seconds)Figure 13: Runtime breakdown. We show a runtime breakdown of our method using FLUX.1 Dev\nmodel as the number of initial candidates (M, left) and the output set size (K, right) is increased. On\nthe left plot, the output size is fixed to 4 and in the right plot the initial candidate size is fixed to 200.\nAcross all settings, the runtime is dominated by the denoising step.\nC.1 Baselines\nIncreasing steps. For FLUX.1 Dev, Stable Diffusion 3 Medium, FLUX.1 Depth, and SynCD, we consider\nthe timesteps 10, 20, 30, 40, and 50. For the distilled model, FLUX.1 Schnell, we consider the timesteps 1, 2, 4,\nand 8.\nVarying CFG. For FLUX.1 Dev, FLUX.1 Depth, and SynCD we consider the CFG values 1, 2, 3, 4, and 5.\nFor Stable Diffusion 3 Medium, we consider the CFG values 1, 5, 10, and 15. Note that FLUX.1 Schnell does\nnot use CFG.\nInterval guidance. For FLUX.1 Dev, FLUX.1 Depth, Stable Diffusion 3 Medium, and SynCD we consider\nthe guidance intervals [0.9,0.1],[0.8,0.2],[0.7,0.3], and[0.6,0.4]. Note that FLUX.1 Schnell does not use\nCFG, and this baseline is not applicable.\nParticle guidance. For the Particle Guidance baseline, we consider coefficient values 0, 10, 50, 100, and\n200. Note that this baseline significantly increases the memory consumption during inference.\nInference diffusion scaling [ 2].Figure 8 of the main paper shows a comparison to Inference Diffusion\nScaling [ 2], a concurrent work, that shows an improvement in the quality of samples. We follow the results in their\npaper and use random search as the strategy. For a fair comparison, we use the same CLIP text-image-similarity\nas the verifier.\nGroup inference (ours). In Figures 5, 17, and 6 of the main paper, we vary the \u03bbdefined in Equation 4\nwhile keeping the input samples Mfixed. We use M= 128 for FLUX.1 Dev, FLUX.1 Schnell, SD3 (M), and\nSynCD. Note that varying the weighting factor \u03bbdoes not change the runtime, and only shows the trade-off\nbetween the diversity of samples in the generated output set and the individual quality. For FLUX.1 Dev, FLUX.1\nDepth, SynCD we use \u03c1= 0.5. For SD3 (M) we use a higher \u03c1= 0.75, and for timestep distilled model\nFLUX.1 Schnell \u03c1= 0.1in all experiments.\nIn Figures 8, 9 and 10 of the main paper, we want to study the performance at different runtimes, and therefore\nwe fix the weighting factor \u03bb= 1but vary the number of input samples Mfrom 4 to 128.\nChoice of scores. Unless specified otherwise, FLUX.1 Dev, FLUX.1 Schnell, SD3 (M), Flux.1 Depth use\nCLIP text-image similarity as the unary score, and DINO diversity as the binary score. SynCD uses DINO target\nimage similarity as the unary score, and DINO diversity as the binary score across all results. Figure 11 keeps the\nunary score fixed as the CLIP text-image similarity and shows the effects of varying the binary score function.\nC.2 Dataset\nText to image generation. All text-to-image generation results with models FLUX.1 Dev, FLUX.1 Schnell,\nand SD3 (M) use all 553 prompts from the GenEval dataset [41].\nDepth to image generation. All FLUX.1 Depth experiments use depth maps computed using Depth\nAnything Large [44] from 250 images from the validation split of the COCO 2017 dataset [42].\nEncoder-based image customization. For all encoder-based image customization experiments using\nSynCD [46], we use 400 samples from the images in the standard DreamBooth dataset.\n18\n\n--- Page 19 ---\nNFECombined Score\nRuntime(seconds)Combined ScoreFLUX.1 Dev\nNFECombined ScoreRuntime(seconds)Combined ScoreFLUX.1 Schnell\nFigure 14: Effects of different dropping ratio \u03c1.We show the effects of different dropping ratios \u03c1\nfor two different base models: FLUX.1 Dev and FLUX.1 Schnell.\nRuntime(seconds)Combined ScoreInference Cost(NFE)Combined Score\nRuntime(seconds)Combined ScoreInference Cost(NFE)Combined ScoreFLUX.1 DevFLUX.1 Schnell\nGroup Inference (Ours)\nOurs w/o progressive filtering\nFigure 15: Ablating the effect of progressive pruning. Similar for Figure 10 from the main paper,\nwe show the importance of progressive pruning. We report both, the number of function evaluations\n(NFEs) and the wallclock runtime (using one NVIDIA H100). The two plots on the left show the\ncomparison using FLUX.1 Dev. The two plots on the right show FLUX.1 Schnell comparison.\nRuntime(seconds)Combined ScoreFLUX.1 Dev\nRuntime(seconds)Combined ScoreFLUX.1 SchnellOurs w/ efficient decoder\nOurs w/o efficient decoder\nFigure 16: Ablating efficient decoder. We show the effects of using an efficient decoder for decoding\nthe intermediate predictions.\n19\n\n--- Page 20 ---\nImage RewardDepth DiversityFLUX.1 Dev\nBLIP2CLIP DiversityImage RewardDepth DiversityFLUX.1 Schnell\nBLIP2CLIP DiversityImage RewardDepth DiversitySD3 (M)\nBLIP2CLIP Diversity\nIncreasing stepsVarying CFG\nInterval Guidance\nParticle Guidance\nGroup Inference (ours)Figure 17: Quality and Diversity Pareto front with additional metrics. We evaluate the quality\nand diversity of samples generated by different inference strategies for three text-to-image models\n(FLUX.1 Dev, FLUX.1 Schnell, and Stable Diffusion 3 Medium). The top row shows evaluation\nusing Image Reward [ 51] as the quality metric and Depth Diversity as the diversity metric. The\nbottom row uses BLIP2 [ 53] and CLIP Diversity. Note that these metrics are unseen and not used by\nour method.\nTimestepsScore CorrelationTimestepsScore CorrelationUnary Score Correlation at different Intermediate predictionsBinary Score Correlation at different Intermediate predictionsFLUX.1 Depth Sampling Trajectory\ud835\udc65!\u223c\ud835\udc5d(\ud835\udc67)\ud835\udc65\"\ud835\udc65'\".$%\ud835\udc65'\".%\ud835\udc65'\".&%\n(Noise)(Sample)\nTimestepsScore CorrelationTimestepsScore CorrelationUnary Score Correlation at different Intermediate predictionsBinary Score Correlation at different Intermediate predictionsSynCDSampling Trajectory\ud835\udc65!\u223c\ud835\udc5d(\ud835\udc67)\ud835\udc65\"\ud835\udc65'\".$%\ud835\udc65'\".%\ud835\udc65'\".&%\n(Noise)(Sample)\nInput Depth\nInput Image\nFigure 18: Correlation between intermediate and final generation Scores . We follow the same\nprotocol as Figure 3 in the main paper. On the left, we show the reverse diffusion process, visualizing\nthe intermediate predictions \u02c6xtof the final image at different steps for FLUX.1 Depth and SynCD\nmodels. We can observe that the intermediate predictions look similar to true final sample x0for both\nthe models. We further demonstrate this quantitatively by plotting the Spearman correlation of the\nUnary and Binary scores from \u02c6xtversus final x0scores, across different steps.\n20\n\n--- Page 21 ---\nGroup Inference (ours)IncreasingSteps\nInferenceDiffusionScaling \nInference Cost(NFE)Combined ScoreFLUX.1 SchnellInference Cost(NFE)Combined ScoreFLUX.1 Dev\nRuntime(seconds)Combined ScoreFLUX.1 Dev\nRuntime(seconds)Combined ScoreFLUX.1 SchnellFigure 19: Performance at different runtimes. Similar for Figure 5 from the main paper, we show\nthe different ways of allocating inference budget. We report both, the number of function evaluations\n(NFEs) and the wallclock runtime (using one NVIDIA H100).\nLowCFGOursIntervalGuidanceParticleGuidance\u201cA photo of a cell phone and a horse.\u201d\n\u201cA photo of a baseball bat and a giraffe.\u201dLowCFGOursIntervalGuidanceParticleGuidance\nFigure 20: Qualitative results. We compare our proposed method (top row) against alternative\ninference strategies targeting an improved Quality-Diversity tradeoff with FLUX.1 Dev base model.\n21\n\n--- Page 22 ---\n\u201cA photo of a person and a bear.\u201d\n\u201cA photo of a horse and a computer keyboard.\u201d\n\u201cA photo of two backpacks.\n\u201cA photo of a red skis and a brown tie.\u201d\n\u201cA photo of a carrot.\u201d\nI.I.D.Group(ours)\nI.I.D.Group(ours)\nI.I.D.Group(ours)\nI.I.D.Group(ours)\nI.I.D.Group(ours)\nFigure 21: Gallery of results. Qualitative results that show the advantage of our proposed method\nover I.I.D. sampling for depth-to-image generation using FLUX.1 Depth as the base model. The\ninput depth maps and captions are shown on the left and the generated outputs are shown on the right.\nOur method consistently generates outputs that have more diverse backgrounds, styles, and textures.\n22\n\n--- Page 23 ---\n\u201cA photo of a purple wine glass and a black apple.\u201d\u201cA photo of a red cake.\u201d\u201cA photo of a refrigerator.\u201cA photo of a donut below a cat.\u201d\u201csketch \u2026 potted plant\u201dI.I.D.Group(ours)I.I.D.Group(ours)I.I.D.Group(ours)I.I.D.Group(ours)I.I.D.Group(ours)Figure 22: Gallery of results. Qualitative results that show the advantage of our proposed method\nover I.I.D. sampling for depth-to-image generation using FLUX.1 Depth as the base model. The\ninput depth maps and captions are shown on the left and the generated outputs are shown on the right.\nOur method consistently generates outputs that have more diverse backgrounds, styles, and textures.\n23\n\n--- Page 24 ---\n\u201cA red backpack.\u201d\u201cA sneaker with a wheat field in the background.\u201d\u201cA cat in the jungle.\u201d\u201cA dog in the jungle.\u201d\u201cA boot on top of a wooden floor.\u201dI.I.D.Group(ours)I.I.D.Group(ours)I.I.D.Group(ours)I.I.D.Group(ours)I.I.D.Group(ours)Figure 23: Gallery of results. Qualitative results that show the advantage of our proposed method\nover I.I.D. sampling for feedforward customized generation using SynCD [ 46]. The input image\nand captions are shown on the left and the generated outputs are shown on the right. Our method\nconsistently generates outputs that have more diverse backgrounds, object poses, and styles.\n24",
  "project_dir": "artifacts/projects/ScalableGroupInference_Diffusion",
  "communication_dir": "artifacts/projects/ScalableGroupInference_Diffusion/.agent_comm",
  "assigned_at": "2025-08-22T20:52:36.859293",
  "status": "assigned"
}